# VGGT forward output 
predictions = {
    # Core outputs (torch.Tensor)
    "pose_enc": torch.Tensor,        # Shape: [B, S, 9] - encoded camera poses
    "depth": torch.Tensor,           # Shape: [B, S, H, W, 1] - depth maps  
    "world_points": torch.Tensor,    # Shape: [B, S, H, W, 3] - 3D world coordinates
    "world_points_conf": torch.Tensor, # Shape: [B, S, H, W] - 3D point confidence scores
    "images": torch.Tensor,          # Shape: [B, S, 3, H, W] - original input images
    
    # After pose_encoding_to_extri_intri() conversion:
    "extrinsic": torch.Tensor,       # Shape: [B, S, 3, 4] - camera extrinsic matrices
    "intrinsic": torch.Tensor,       # Shape: [B, S, 3, 3] - camera intrinsic matrices
}

extrinsic, intrinsic = pose_encoding_to_extri_intri(predictions["pose_enc"], images.shape[-2:])
predictions["extrinsic"] = extrinsic
predictions["intrinsic"] = intrinsic



# Instant expected format from MASt3R
instantsplat_format = {
    'extrinsics_w2c': np.array,         # [N_views, 4, 4] - 4x4 matrices
    'intrinsics': np.array,             # [N_views, 3, 3]
    'pts3d': np.array,                  # [N_views, H, W, 3]
    'confs': np.array,                  # [N_views, H, W]
    'depthmaps': np.array,              # [N_views, H x W]
    'imgs': np.array,                   # [N_views, H, W, 3] -  HWC format
    'focals': np.array                  # [N_views, 1]
}

python init_geo.py -s /data1/tzz/InstantSplatPP/assets/sora/Art  -m /data1/tzz/InstantSplatPP/results --device cuda:3

python train.py \
-s /data1/tzz/InstantSplatPP/assets/sora/Art \
-m vggt_art \
-r 1 \
--n_views 3 \
--iterations 3000 \
--pp_optimizer \
--optim_pose